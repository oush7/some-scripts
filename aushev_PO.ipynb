{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Языковая модель (LLM) - это статистическая модель, которая используется для предсказания вероятности последовательности слов или фраз в рамках определенного языка. Она используется в различных областях обработки естественного языка, таких как машинный перевод, распознавание речи, исправление ошибок и другие. Языковая модель помогает компьютеру понять и генерировать естественный язык, учитывая контекст и вероятность последовательности слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При работе LLM используют разные подходы к генерации текста, самый стандартный это autoregressive sampling, однако существуют и другие более продвинутые методы, которые помогают ускорить работу модели, в данной работе будет реализован стандартный метод сэмплировния autoregressive sampling, а также более умный метод speculative sampling. Будет проведен сравнительный анализ эффективности двух методов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже представлен стандартный алгоритм сэмплирования языковых моделей, каждый следующий токен последовательности генерируется с учетом предыдущих."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"654\" alt=\"image\" src=\"https://github.com/markovka17/dla/assets/20357655/db624e40-d4f0-4e36-88e7-b58a6c646738\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем этот алгоритм для модели GPT-NEO-1.3B, на вход дадим модели предложение The quick brown fox jumps, посмотрим что дальше сгенерирует модель для этого предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b308b1ba40a4b44859692dd92f6b146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0afdafdbfd402380b88ec76afc6f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c78264020f04bac9f138a9f0d1ee771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef81f632d6e425ca9f670a08c05118e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ec4348ae5d40568056ee6f34cd3a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/4.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c15d1b42b4540f8973914f3878832bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9708d5392f1543b3b00b1aa0511d2c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9e389e4a84474fbb847ca4e5fb446e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " The quick brown fox jumps over the lazy dog\" \"♪ Ooh, ooh, ooh\" \"♪ A little bit ooh, a little bit frack, a little bit frack ♪\" \"♪ A little bit ooh, a little bit frack, a little bit frack.\" \"♪ ♪\" \"Why don't you humans inside?\" \"♪ Hey, I cannae see it laff ya, take my advice\" \"\n"
     ]
    }
   ],
   "source": [
    "# Ensure that your device is set correctly (GPU or CPU)\n",
    "device = 'cuda'\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "model = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-1.3B', torch_dtype=torch.float16).to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Prepare a text prompt\n",
    "text_prompt = [\"The quick brown fox jumps\"]\n",
    "inputs = tokenizer(text_prompt, return_tensors='pt').to(device)  # Tokenize the text prompt and convert to tensor\n",
    "\n",
    "max_length = 100  # Maximum length of the generated text\n",
    "temperature = 1.0  # Sampling temperature, higher values mean more randomness\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    output_sequence = inputs['input_ids']\n",
    "    for _ in range(max_length - inputs['input_ids'].size(1)):\n",
    "        # Predict the next token\n",
    "        logits = model(output_sequence).logits[:, -1, :]\n",
    "        \n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # Sample the next token from the probability distribution\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "        \n",
    "        # Append the predicted token to the output sequence\n",
    "        output_sequence = torch.cat([output_sequence, next_token], dim=1)\n",
    "\n",
    "        # Check if the end-of-sequence token (EOS) was generated\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output_sequence.squeeze(), skip_special_tokens=True)\n",
    "print(\"Generated text:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили сгенерированный текст, видим, что сначала модель уловила смысл предложения и корректно его продолжила, а далее уже начала уходить из контекста и придумывать свое продолжение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем модель побольше, а именно GPTJ-6B, у которой 6 миллиардов обучаемых параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb5aba66e5a41b382ff79b0628e03a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/930 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77996d78dac04068b06c7b1675af822a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/24.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aushevis/speech_course/speech_course/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "large_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на слои этой модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJForCausalLM(\n",
       "  (transformer): GPTJModel(\n",
       "    (wte): Embedding(50400, 4096)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-27): 28 x GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=50400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прошлый подход для сэмплирования работает, но существуют методы более быстрой генерации, одним из таких является speculative sampling, предложенный в статье https://arxiv.org/pdf/2302.01318"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея speculative sampling заключается в следующем, мы берем две языковых модели:\n",
    "\n",
    "Маленькую быструю драфт модель (например, EleutherAI/gpt-neo-1.3B, у которой только 1.3 миллиарда параметров)\n",
    "И большую, медленную таргет модель (в нашем случае это EleutherAI/gpt-j-6B), которой мы хотим генерировать ответ\n",
    "\n",
    "Идея заключается в том, что драфт модель предполагает, что результатом генерации таргет модели будут какие-то токены, в то время как целевая модель определяет, сколько из этих токенов мы должны принять. Вот краткое описание алгоритма:\n",
    "\n",
    "Драфт модель декодирует токены обычным способом авторегрессии.\n",
    "Мы получаем вероятностные выходные данные таргет и драфт моделей для новой предсказанной последовательности.\n",
    "Мы сравниваем вероятности таргет и драфт моделей, чтобы определить, сколько токенов мы хотим сохранить, основываясь на некоторых критериях отклонения. Если токен отклонен, мы проводим повторную выборку, используя комбинацию двух распределений, и больше не принимаем токены.\n",
    "Если все токены приняты, мы можем выбрать дополнительный конечный токен из выходных данных вероятности таргет модели.\n",
    "\n",
    "Благодаря такому подходу мы меньше используем большую таргет модель, а большинство вычислений производит маленькая модель, благодаря чему и происходит выигрыш по времени"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"635\" alt=\"image\" src=\"https://github.com/markovka17/dla/assets/20357655/3954894d-8735-4f92-a835-d04eac74f190\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем speculative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def speculative_sampling(x, draft_model, target_model, N, K, use_cache=True, debug=True):\n",
    "    # NOTE: paper indexes arrays starting from 1, python indexes from 0, so\n",
    "    # we have to add an extra -1 term when indexing using n, T, or t\n",
    "    with torch.no_grad():\n",
    "        n = len(x)\n",
    "        one = torch.tensor(1)\n",
    "        output_sequence = x['input_ids']\n",
    "        generated_tokens = 0\n",
    "        accepted_tokens = 0\n",
    "        while output_sequence.shape[1] < N:\n",
    "            # Step 1: auto-regressive decode K tokens from draft model and get final p\n",
    "            # logits_draft = torch.rand(1)\n",
    "            res = draft_model.generate(output_sequence, \n",
    "                                       max_new_tokens=K, \n",
    "                                       output_scores=True, \n",
    "                                       return_dict_in_generate=True, \n",
    "                                       do_sample=True,\n",
    "                                       pad_token_id=tokenizer.eos_token_id, use_cache=use_cache)\n",
    "            logits_draft = torch.concat(res.scores).unsqueeze(0)\n",
    "            output_sequence = res.sequences\n",
    "            generated_tokens += logits_draft.shape[1]\n",
    "            \n",
    "            # Step 2: target model forward passes on x_draft\n",
    "            seq_len = output_sequence.shape[1]\n",
    "            draft_len = logits_draft.shape[1]\n",
    "            logits_target = target_model(output_sequence).logits[:, seq_len - draft_len - 1:, :]\n",
    "\n",
    "            # Step 3: append draft tokens based on rejection criterion and resample\n",
    "            # a token on rejection\n",
    "            all_accepted = True\n",
    "            probs_draft = torch.nn.functional.softmax(logits_draft[:, :, :], dim=-1)\n",
    "            probs_target = torch.nn.functional.softmax(logits_target[:, :, :probs_draft.shape[2]], dim=-1)\n",
    "            for t in range(probs_draft.shape[1]):\n",
    "                r = torch.rand(output_sequence.shape[0], device=device)\n",
    "                predicted_token = output_sequence[:, seq_len - draft_len + t - 1]\n",
    "\n",
    "                if torch.all(r <= torch.min(one, probs_target[:, t , predicted_token] / probs_draft[:, t, predicted_token])):\n",
    "                    accepted_tokens += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    all_accepted = False\n",
    "                    probabilities = torch.nn.functional.relu(probs_target[:, t,:] - probs_draft[:, t, :])\n",
    "                    probabilities = probabilities / probabilities.sum()\n",
    "                    next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "                    output_sequence = output_sequence[:, :seq_len - draft_len + t]\n",
    "                    output_sequence = torch.cat([output_sequence, next_token], dim=1)\n",
    "                    break\n",
    "            \n",
    "            # Step 4: if all draft tokens were accepted, sample a final token\n",
    "            if all_accepted:\n",
    "                # print(\"ALL accepted\")\n",
    "                probabilities = torch.nn.functional.softmax(logits_target[:, -1 ,:], dim=-1)\n",
    "                next_token = torch.multinomial(probs_target[:, -1, :], num_samples=1)\n",
    "                output_sequence = torch.cat([output_sequence, next_token], dim=1)\n",
    "                \n",
    "            assert n == len(x), f\"{n} {len(x)}\"\n",
    "        acceptance_rate = accepted_tokens / generated_tokens\n",
    "    return output_sequence, acceptance_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем работу алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceptence rate:  0.9333333333333333\n",
      "Generated text:\n",
      " The quick brown fox jumps over the lazy dog….\n",
      "\n",
      "Tag Archive\n",
      "\n",
      "The quick brown fox jumps over the lazy dog in the morning, and you never notice it’s missing. “D” is for diabetic shock.\n",
      "\n",
      "Nothing is more annoying and embarrassing than having to admit that you have a disease. I went to a medical conference a few weeks ago and was surprised that I came back with a.1% chance that I have Diabetic Shock.\n",
      "\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "output_sequence, acceptance_rate = speculative_sampling(inputs, model, large_model, 100, 3)\n",
    "generated_text = tokenizer.decode(output_sequence.squeeze(), skip_special_tokens=True)\n",
    "print('Acceptence rate: ', acceptance_rate)\n",
    "print(\"Generated text:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним эффективность стандартного подхода и speculative sampling, для этого будем использовать torch.profiler для оценки производительности CPU и GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Callable, Dict, Sequence\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm import trange\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.amp import autocast\n",
    "import torch.backends.xnnpack\n",
    "from torch.profiler import profile, schedule, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProfilerConfig:\n",
    "    scheduler: Optional[Callable[[int], int]] = field(default=None, metadata={\"omegaconf_ignore\": True})\n",
    "    num_steps: int = field(init=False)\n",
    "    amp: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.scheduler is None:\n",
    "            self.scheduler = {\"skip_first\": 0, \"wait\": 1, \"warmup\": 1, \"active\": 3, \"repeat\": 0}\n",
    "\n",
    "        self.num_steps = sum([v for k, v in self.scheduler.items() if k != \"repeat\"])\n",
    "        self.scheduler = schedule(**self.scheduler)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def profile_model(model: nn.Module, input: Dict[str, torch.Tensor], config: ProfilerConfig):\n",
    "    training_state = model.training\n",
    "    model.eval()\n",
    "\n",
    "    with autocast(dtype=torch.float16, device_type=\"cuda\", enabled=config.amp), profile(\n",
    "        activities=[ProfilerActivity.CUDA ],\n",
    "        schedule=config.scheduler\n",
    "    ) as p:\n",
    "        for _ in range(config.num_steps):\n",
    "            _ = model(**input)\n",
    "            p.step()\n",
    "\n",
    "    model.train(training_state)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultModel(nn.Module):\n",
    "    def __init__(self, model, max_length):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, text_prompt):\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            input = tokenizer(text_prompt, return_tensors='pt').to(device)\n",
    "            output_sequence = input['input_ids']\n",
    "            for _ in range(self.max_length - input['input_ids'].size(1)):\n",
    "                # Predict the next token\n",
    "                logits = model(output_sequence).logits[:, -1, :]\n",
    "                \n",
    "                # Sample the next token from the probability distribution\n",
    "                probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "                \n",
    "                # Append the predicted token to the output sequence\n",
    "                output_sequence = torch.cat([output_sequence, next_token], dim=1)\n",
    "\n",
    "                # Check if the end-of-sequence token (EOS) was generated\n",
    "                # if next_token.item() == tokenizer.eos_token_id:\n",
    "                #     break\n",
    "            return output_sequence\n",
    "        \n",
    "class SpsModel(nn.Module):\n",
    "    def __init__(self, draft_model, target_model, max_length):\n",
    "        super().__init__()\n",
    "        self.draft_model = draft_model\n",
    "        self.target_model = target_model\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, text_prompt, K):\n",
    "        return speculative_sampling(text_prompt, self.draft_model, self.target_model, self.max_length, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала посмотрим сколько будет генерироваться текст длины 512 для стандартного метода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-03 09:54:55 1597137:1597137 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2024-05-03 09:55:52 1597137:1597137 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2024-05-03 09:55:54 1597137:1597137 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        cudaMemcpyAsync        16.13%        3.629s        16.13%        3.629s      91.753us       0.000us         0.00%       0.000us       0.000us         39552  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      33.608ms         0.08%      33.608ms       0.921us         36510  \n",
      "                                  cudaStreamSynchronize        42.47%        9.554s        42.47%        9.554s     241.547us       0.000us         0.00%       0.000us       0.000us         39552  \n",
      "                                       cudaLaunchKernel        39.13%        8.803s        39.13%        8.803s       8.402us       0.000us         0.00%       0.000us       0.000us       1047762  \n",
      "void (anonymous namespace)::elementwise_kernel_with_...         0.00%       0.000us         0.00%       0.000us       0.000us       5.734ms         0.01%       5.734ms       1.885us          3042  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us     557.000us         0.00%     557.000us       7.736us            72  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us        1.704s         4.05%        1.704s      15.346us        111033  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.176ms         0.01%       3.176ms       2.088us          1521  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.078ms         0.01%       3.078ms       2.024us          1521  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       5.845ms         0.01%       5.845ms       3.843us          1521  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.893ms         0.01%       3.893ms       2.560us          1521  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     847.802ms         2.01%     847.802ms      22.296us         38025  \n",
      "void at::native::(anonymous namespace)::vectorized_l...         0.00%       0.000us         0.00%       0.000us       0.000us     591.512ms         1.41%     591.512ms       7.937us         74529  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.98%     220.114ms         0.98%     220.114ms       0.281us       0.000us         0.00%       0.000us       0.000us        784643  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_ldg8_f2f_stages_64...         0.00%       0.000us         0.00%       0.000us       0.000us      59.948ms         0.14%      59.948ms      25.231us          2376  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     783.058ms         1.86%     783.058ms      10.726us         73008  \n",
      "void gemmSN_TN_kernel<float, 128, 16, 2, 4, 6, 7, fa...         0.00%       0.000us         0.00%       0.000us       0.000us       1.088ms         0.00%       1.088ms       3.778us           288  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     911.864ms         2.17%     911.864ms      24.980us         36504  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us        1.197s         2.84%        1.197s      32.780us         36504  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us     661.000us         0.00%     661.000us       2.295us           288  \n",
      "                       ampere_fp16_sgemm_fp16_128x32_nn         0.00%       0.000us         0.00%       0.000us       0.000us     628.000us         0.00%     628.000us       4.361us           144  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     257.779ms         0.61%     257.779ms       7.062us         36504  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_rel...         0.00%       0.000us         0.00%       0.000us       0.000us     101.671ms         0.24%     101.671ms      58.837us          1728  \n",
      "void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us      72.426ms         0.17%      72.426ms      91.447us           792  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us        2.333s         5.54%        2.333s      21.305us        109512  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     849.782ms         2.02%     849.782ms      23.279us         36504  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     824.575ms         1.96%     824.575ms      22.589us         36504  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     804.672ms         1.91%     804.672ms      22.043us         36504  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us        1.222s         2.90%        1.222s      33.485us         36504  \n",
      "void cutlass::Kernel<cutlass_75_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us      39.580ms         0.09%      39.580ms     527.733us            75  \n",
      "void at::native::(anonymous namespace)::cunn_SoftMax...         0.00%       0.000us         0.00%       0.000us       0.000us      37.568ms         0.09%      37.568ms      24.700us          1521  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      15.691ms         0.04%      15.691ms      10.316us          1521  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.306ms         0.01%       6.306ms       2.073us          3042  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      15.211ms         0.04%      15.211ms      10.001us          1521  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.064ms         0.01%       3.064ms       2.014us          1521  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us       3.039ms         0.01%       3.039ms       0.999us          3042  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      12.722ms         0.03%      12.722ms       8.364us          1521  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.093ms         0.01%       3.093ms       2.034us          1521  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       3.394ms         0.01%       3.394ms       2.231us          1521  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       5.366ms         0.01%       5.366ms       3.528us          1521  \n",
      "                                  cudaStreamIsCapturing         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us          1521  \n",
      "void at::native::(anonymous namespace)::distribution...         0.00%       0.000us         0.00%       0.000us       0.000us       6.396ms         0.02%       6.396ms       4.205us          1521  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.236ms         0.01%       4.236ms       2.785us          1521  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      24.480ms         0.06%      24.480ms      16.095us          1521  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       4.629ms         0.01%       4.629ms       3.043us          1521  \n",
      "void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us     865.000us         0.00%     865.000us       3.003us           288  \n",
      "void gemmSN_TN_kernel<float, 128, 16, 2, 4, 8, 9, fa...         0.00%       0.000us         0.00%       0.000us       0.000us       1.010ms         0.00%       1.010ms       3.507us           288  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x...         0.00%       0.000us         0.00%       0.000us       0.000us      10.412ms         0.02%      10.412ms      24.102us           432  \n",
      "void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us     442.000us         0.00%     442.000us       3.069us           144  \n",
      "void gemmSN_TN_kernel<float, 128, 16, 2, 4, 10, 11, ...         0.00%       0.000us         0.00%       0.000us       0.000us     460.000us         0.00%     460.000us       3.194us           144  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us       1.183ms         0.00%       1.183ms       2.054us           576  \n",
      "void cutlass::Kernel<cutlass_75_tensorop_f16_s1688ge...         0.00%       0.000us         0.00%       0.000us       0.000us      28.173ms         0.07%      28.173ms      10.033us          2808  \n",
      "void gemmSN_TN_kernel<float, 128, 16, 2, 4, 14, 15, ...         0.00%       0.000us         0.00%       0.000us       0.000us     575.000us         0.00%     575.000us       3.993us           144  \n",
      "                                   cudaFuncSetAttribute         0.02%       5.606ms         0.02%       5.606ms       0.050us       0.000us         0.00%       0.000us       0.000us        112989  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_f16_s16816g...         0.00%       0.000us         0.00%       0.000us       0.000us       6.705ms         0.02%       6.705ms      93.125us            72  \n",
      "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us      41.625ms         0.10%      41.625ms      14.015us          2970  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us     148.242ms         0.35%     148.242ms      26.396us          5616  \n",
      "                                ampere_sgemm_128x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us        2.403s         5.71%        2.403s      67.424us         35640  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us       2.333ms         0.01%       2.333ms       2.025us          1152  \n",
      "void cutlass::Kernel<cutlass_75_tensorop_f16_s1688ge...         0.00%       0.000us         0.00%       0.000us       0.000us     307.022ms         0.73%     307.022ms      36.137us          8496  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_rel...         0.00%       0.000us         0.00%       0.000us       0.000us     418.779ms         0.99%     418.779ms      61.876us          6768  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_ldg8_relu_f2f_stag...         0.00%       0.000us         0.00%       0.000us       0.000us     330.775ms         0.79%     330.775ms      95.710us          3456  \n",
      "void cutlass::Kernel<cutlass_75_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us       4.763ms         0.01%       4.763ms     529.222us             9  \n",
      "void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us       2.984ms         0.01%       2.984ms       3.454us           864  \n",
      "void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us     432.000us         0.00%     432.000us       3.000us           144  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us       6.912ms         0.02%       6.912ms       3.000us          2304  \n",
      "void cutlass::Kernel<cutlass_75_tensorop_f16_s1688ge...         0.00%       0.000us         0.00%       0.000us       0.000us      66.541ms         0.16%      66.541ms     693.135us            96  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us     119.815ms         0.28%     119.815ms      26.414us          4536  \n",
      "void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us      25.491ms         0.06%      25.491ms      11.064us          2304  \n",
      "void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us       8.267ms         0.02%       8.267ms       5.468us          1512  \n",
      "void cutlass::Kernel<cutlass_75_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us       1.454ms         0.00%       1.454ms       5.049us           288  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_f16_s16816g...         0.00%       0.000us         0.00%       0.000us       0.000us       4.235ms         0.01%       4.235ms      29.410us           144  \n",
      "void splitKreduce_kernel<32, 16, int, __half, __half...         0.00%       0.000us         0.00%       0.000us       0.000us     715.000us         0.00%     715.000us       4.965us           144  \n",
      "ampere_fp16_s16816gemm_fp16_128x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us     119.822ms         0.28%     119.822ms      30.818us          3888  \n",
      "void splitKreduce_kernel<32, 16, int, __half, __half...         0.00%       0.000us         0.00%       0.000us       0.000us     125.091ms         0.30%     125.091ms       7.425us         16848  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us      20.018ms         0.05%      20.018ms       4.344us          4608  \n",
      "void cutlass::Kernel<cutlass_75_tensorop_f16_s1688ge...         0.00%       0.000us         0.00%       0.000us       0.000us      18.688ms         0.04%      18.688ms       8.111us          2304  \n",
      "                                        cudaMemsetAsync         1.26%     283.976ms         1.26%     283.976ms       6.919us       0.000us         0.00%       0.000us       0.000us         41040  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us      39.218ms         0.09%      39.218ms       0.956us         41040  \n",
      "ampere_fp16_s16816gemm_fp16_128x64_ldg8_relu_f2f_sta...         0.00%       0.000us         0.00%       0.000us       0.000us     317.173ms         0.75%     317.173ms      68.831us          4608  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_f16_s16816g...         0.00%       0.000us         0.00%       0.000us       0.000us     809.045ms         1.92%     809.045ms      52.755us         15336  \n",
      "void cutlass::Kernel<cutlass_75_tensorop_f16_s1688ge...         0.00%       0.000us         0.00%       0.000us       0.000us     237.638ms         0.56%     237.638ms     990.158us           240  \n",
      "      ampere_s16816gemm_fp16_128x64_ldg8_stages_32x6_tn         0.00%       0.000us         0.00%       0.000us       0.000us     242.734ms         0.58%     242.734ms      43.222us          5616  \n",
      "void splitKreduce_kernel<32, 16, int, float, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us      40.772ms         0.10%      40.772ms       7.260us          5616  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize64x96x...         0.00%       0.000us         0.00%       0.000us       0.000us      17.780ms         0.04%      17.780ms      27.438us           648  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_f16_s16816g...         0.00%       0.000us         0.00%       0.000us       0.000us     273.491ms         0.65%     273.491ms     105.514us          2592  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_f16_s16816g...         0.00%       0.000us         0.00%       0.000us       0.000us        7.915s        18.80%        7.915s     167.326us         47304  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_f16_s16816g...         0.00%       0.000us         0.00%       0.000us       0.000us       7.778ms         0.02%       7.778ms      12.003us           648  \n",
      "ampere_fp16_s16816gemm_fp16_128x64_ldg8_f2f_stages_3...         0.00%       0.000us         0.00%       0.000us       0.000us     576.587ms         1.37%     576.587ms      31.405us         18360  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us     105.765ms         0.25%     105.765ms      11.476us          9216  \n",
      "ampere_fp16_s16816gemm_fp16_128x64_ldg8_relu_f2f_sta...         0.00%       0.000us         0.00%       0.000us       0.000us     642.066ms         1.53%     642.066ms      65.570us          9792  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_f16_s16816g...         0.00%       0.000us         0.00%       0.000us       0.000us     412.826ms         0.98%     412.826ms     119.452us          3456  \n",
      "void cutlass::Kernel<cutlass_75_tensorop_f16_s1688ge...         0.00%       0.000us         0.00%       0.000us       0.000us        1.775s         4.22%        1.775s       2.476ms           717  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_f16_s16816g...         0.00%       0.000us         0.00%       0.000us       0.000us       1.152ms         0.00%       1.152ms       8.000us           144  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_f16_s16816g...         0.00%       0.000us         0.00%       0.000us       0.000us      42.397ms         0.10%      42.397ms      16.357us          2592  \n",
      "void cutlass::Kernel<cutlass_75_tensorop_f16_s1688ge...         0.00%       0.000us         0.00%       0.000us       0.000us     111.963ms         0.27%     111.963ms      26.357us          4248  \n",
      "void cutlass::Kernel<cutlass_80_tensorop_f16_s16816g...         0.00%       0.000us         0.00%       0.000us       0.000us     255.535ms         0.61%     255.535ms      33.482us          7632  \n",
      "ampere_fp16_s16816gemm_fp16_128x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us        1.473s         3.50%        1.473s      53.279us         27648  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us     976.679ms         2.32%     976.679ms      53.196us         18360  \n",
      "void cutlass::Kernel<cutlass_75_tensorop_f16_s1688ge...         0.00%       0.000us         0.00%       0.000us       0.000us     958.696ms         2.28%     958.696ms       2.497ms           384  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 22.496s\n",
      "Self CUDA time total: 42.095s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = profile_model(DefaultModel(large_model, 512), {'text_prompt': text_prompt}, ProfilerConfig(amp=False))\n",
    "print(p.key_averages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как все вычисления производились на GPU, то нас интересует именно последняя строчка выхода профайлера, у обычного метода заняло 42 секунды на работу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем speculative sampling для K=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-03 10:00:06 1597137:1597137 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2024-05-03 10:01:21 1597137:1597137 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2024-05-03 10:01:24 1597137:1597137 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                       cudaLaunchKernel        60.17%       14.365s        60.17%       14.365s       7.923us       0.000us         0.00%       0.000us       0.000us       1813069  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.035ms         0.03%       8.035ms       2.013us          3991  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     476.000us         0.00%     476.000us       3.967us           120  \n",
      "                                        cudaMemcpyAsync        32.92%        7.860s        32.92%        7.860s      79.160us       0.000us         0.00%       0.000us       0.000us         99292  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us      35.135ms         0.11%      35.135ms       1.002us         35066  \n",
      "                                  cudaStreamSynchronize         5.17%        1.235s         5.17%        1.235s      14.294us       0.000us         0.00%       0.000us       0.000us         86426  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      25.860ms         0.08%      25.860ms       1.633us         15840  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       4.393ms         0.01%       4.393ms       2.034us          2160  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       6.664ms         0.02%       6.664ms       3.085us          2160  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     720.000us         0.00%     720.000us       2.000us           360  \n",
      "                                    cudaPeekAtLastError         0.01%       2.664ms         0.01%       2.664ms       0.034us       0.000us         0.00%       0.000us       0.000us         77752  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us      25.615ms         0.08%      25.615ms       1.642us         15600  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         1.18%     281.935ms         1.18%     281.935ms       0.451us       0.000us         0.00%       0.000us       0.000us        625030  \n",
      "                                 cudaDeviceGetAttribute         0.00%     253.000us         0.00%     253.000us       0.013us       0.000us         0.00%       0.000us       0.000us         19440  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us      40.395ms         0.13%      40.395ms       2.589us         15600  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      34.806ms         0.11%      34.806ms       2.705us         12866  \n",
      "void compute_cuda_kernel<long>(long*, long*, long*, ...         0.00%       0.000us         0.00%       0.000us       0.000us      28.388ms         0.09%      28.388ms       2.075us         13680  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us     705.000us         0.00%     705.000us       2.938us           240  \n",
      "void (anonymous namespace)::elementwise_kernel_with_...         0.00%       0.000us         0.00%       0.000us       0.000us     660.000us         0.00%     660.000us       1.833us           360  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.798ms         0.02%       7.798ms       2.014us          3871  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.025ms         0.01%       4.025ms       2.096us          1920  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       9.803ms         0.03%       9.803ms       2.713us          3614  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     740.940ms         2.37%     740.940ms       4.932us        150240  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     264.000us         0.00%     264.000us       2.200us           120  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     456.000us         0.00%     456.000us       3.800us           120  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     304.000us         0.00%     304.000us       2.533us           120  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     347.522ms         1.11%     347.522ms       6.351us         54720  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     541.000us         0.00%     541.000us       4.508us           120  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.908ms         0.01%       3.908ms       2.035us          1920  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.513ms         0.02%       5.513ms       2.702us          2040  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.106ms         0.03%       8.106ms       2.047us          3960  \n",
      "void at::native::(anonymous namespace)::vectorized_l...         0.00%       0.000us         0.00%       0.000us       0.000us     561.856ms         1.79%     561.856ms       5.759us         97560  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_ldg8_f2f_stages_64...         0.00%       0.000us         0.00%       0.000us       0.000us      10.978ms         0.04%      10.978ms      25.412us           432  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     657.450ms         2.10%     657.450ms       7.125us         92280  \n",
      "void gemmSN_TN_kernel<float, 128, 16, 2, 4, 6, 7, fa...         0.00%       0.000us         0.00%       0.000us       0.000us     269.000us         0.00%     269.000us       3.736us            72  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      37.239ms         0.12%      37.239ms       0.725us         51360  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     290.013ms         0.93%     290.013ms       5.866us         49440  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     270.492ms         0.86%     270.492ms       5.870us         46080  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us     683.000us         0.00%     683.000us       2.189us           312  \n",
      "                       ampere_fp16_sgemm_fp16_128x32_nn         0.00%       0.000us         0.00%       0.000us       0.000us     337.000us         0.00%     337.000us       4.681us            72  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     109.223ms         0.35%     109.223ms       5.550us         19680  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_rel...         0.00%       0.000us         0.00%       0.000us       0.000us      19.781ms         0.06%      19.781ms      58.872us           336  \n",
      "void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us        3.133s        10.00%        3.133s      24.140us        129768  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     910.436ms         2.91%     910.436ms       6.138us        148320  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     312.233ms         1.00%     312.233ms       6.315us         49440  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     310.630ms         0.99%     310.630ms       6.283us         49440  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     307.997ms         0.98%     307.997ms       6.230us         49440  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     417.943ms         1.33%     417.943ms       8.454us         49440  \n",
      "void cutlass::Kernel<cutlass_75_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us       5.259ms         0.02%       5.259ms     525.900us            10  \n",
      "                                        cudaMemsetAsync         0.53%     127.036ms         0.53%     127.036ms       6.483us       0.000us         0.00%       0.000us       0.000us         19595  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us      18.955ms         0.06%      18.955ms       0.967us         19595  \n",
      "void at::native::mbtopk::fill<unsigned int, unsigned...         0.00%       0.000us         0.00%       0.000us       0.000us       3.869ms         0.01%       3.869ms       2.015us          1920  \n",
      "void at::native::mbtopk::radixFindKthValues<c10::Hal...         0.00%       0.000us         0.00%       0.000us       0.000us      21.320ms         0.07%      21.320ms       5.552us          3840  \n",
      "void at::native::mbtopk::computeBlockwiseWithinKCoun...         0.00%       0.000us         0.00%       0.000us       0.000us       8.136ms         0.03%       8.136ms       2.119us          3840  \n",
      "void at::native::mbtopk::computeBlockwiseKthCounts<u...         0.00%       0.000us         0.00%       0.000us       0.000us       3.867ms         0.01%       3.867ms       2.014us          1920  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       4.573ms         0.01%       4.573ms       1.191us          3840  \n",
      "void at_cuda_detail::cub::DeviceScanByKeyKernel<at_c...         0.00%       0.000us         0.00%       0.000us       0.000us      12.295ms         0.04%      12.295ms       3.202us          3840  \n",
      "void at::native::mbtopk::gatherTopK<c10::Half, unsig...         0.00%       0.000us         0.00%       0.000us       0.000us      10.580ms         0.03%      10.580ms       5.510us          1920  \n",
      "void at::native::radixSortKVInPlace<-2, -1, 32, 4, c...         0.00%       0.000us         0.00%       0.000us       0.000us       9.724ms         0.03%       9.724ms       5.065us          1920  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       5.835ms         0.02%       5.835ms       3.039us          1920  \n",
      "void at::native::(anonymous namespace)::cunn_SoftMax...         0.00%       0.000us         0.00%       0.000us       0.000us      65.206ms         0.21%      65.206ms      31.964us          2040  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      17.498ms         0.06%      17.498ms       9.114us          1920  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.735ms         0.02%       7.735ms       2.014us          3840  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      17.029ms         0.05%      17.029ms       8.869us          1920  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.141ms         0.01%       4.141ms       2.030us          2040  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      13.880ms         0.04%      13.880ms       7.229us          1920  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.864ms         0.01%       3.864ms       2.013us          1920  \n",
      "                                  cudaStreamIsCapturing         0.01%       2.465ms         0.01%       2.465ms       0.703us       0.000us         0.00%       0.000us       0.000us          3506  \n",
      "void at::native::(anonymous namespace)::distribution...         0.00%       0.000us         0.00%       0.000us       0.000us       7.630ms         0.02%       7.630ms       3.974us          1920  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.167ms         0.01%       4.167ms       2.170us          1920  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      27.264ms         0.09%      27.264ms      14.200us          1920  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.880ms         0.01%       3.880ms       2.021us          1920  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.877ms         0.01%       3.877ms       2.019us          1920  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.849ms         0.01%       3.849ms       2.005us          1920  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.893ms         0.01%       3.893ms       2.028us          1920  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      10.158ms         0.03%      10.158ms       2.565us          3960  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.723ms         0.02%       6.723ms       1.167us          5760  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.728ms         0.02%       7.728ms       2.013us          3840  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.920ms         0.01%       1.920ms       2.000us           960  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.859ms         0.01%       3.859ms       2.010us          1920  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       3.940ms         0.01%       3.940ms       2.052us          1920  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       5.867ms         0.02%       5.867ms       3.056us          1920  \n",
      "void splitKreduce_kernel<32, 16, int, __half, __half...         0.00%       0.000us         0.00%       0.000us       0.000us     359.455ms         1.15%     359.455ms       2.735us        131416  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      74.959ms         0.24%      74.959ms      13.014us          5760  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us     237.794ms         0.76%     237.794ms       5.504us         43200  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us      28.922ms         0.09%      28.922ms       3.082us          9384  \n",
      "void gemv2T_kernel_val<int, int, __half, __half, __h...         0.00%       0.000us         0.00%       0.000us       0.000us        4.978s        15.90%        4.978s      57.621us         86400  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us        3.716s        11.86%        3.716s      86.007us         43200  \n",
      "void gemv2T_kernel_val<int, int, __half, __half, __h...         0.00%       0.000us         0.00%       0.000us       0.000us     920.046ms         2.94%     920.046ms     511.137us          1800  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us     968.000us         0.00%     968.000us       2.017us           480  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     636.047ms         2.03%     636.047ms       7.887us         80640  \n",
      "void gemv2N_kernel<int, int, __half, __half, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us     158.000us         0.00%     158.000us       2.194us            72  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     970.000us         0.00%     970.000us       2.021us           480  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us       2.465ms         0.01%       2.465ms       2.014us          1224  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us       3.478ms         0.01%       3.478ms       2.008us          1732  \n",
      "void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us       1.936ms         0.01%       1.936ms       3.293us           588  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       1.437ms         0.00%       1.437ms      11.975us           120  \n",
      "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us       5.756ms         0.02%       5.756ms      16.636us           346  \n",
      "ampere_fp16_s16816gemm_fp16_128x64_ldg8_f2f_stages_6...         0.00%       0.000us         0.00%       0.000us       0.000us     107.507ms         0.34%     107.507ms      95.988us          1120  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      66.227ms         0.21%      66.227ms       2.190us         30240  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 23.875s\n",
      "Self CUDA time total: 31.320s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = profile_model(SpsModel(model, large_model, 512), {'text_prompt': inputs, 'K':16}, ProfilerConfig(amp=False))\n",
    "print(p.key_averages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили большой прирост в скорости, более чем на 10 секунд, получили ускорение на 25%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь возьмем K=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-03 10:06:58 1597137:1597137 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2024-05-03 10:08:39 1597137:1597137 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2024-05-03 10:08:42 1597137:1597137 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                       cudaLaunchKernel        69.14%       19.911s        69.14%       19.911s       7.819us       0.000us         0.00%       0.000us       0.000us       2546654  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      11.938ms         0.03%      11.938ms       2.007us          5949  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     369.000us         0.00%     369.000us       4.055us            91  \n",
      "                                        cudaMemcpyAsync        23.19%        6.679s        23.19%        6.679s      54.438us       0.000us         0.00%       0.000us       0.000us        122689  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us      31.432ms         0.08%      31.432ms       1.002us         31360  \n",
      "                                  cudaStreamSynchronize         3.74%        1.077s         3.74%        1.077s      10.090us       0.000us         0.00%       0.000us       0.000us        106708  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      22.682ms         0.06%      22.682ms       1.684us         13468  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       6.264ms         0.02%       6.264ms       2.025us          3094  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       9.446ms         0.03%       9.446ms       3.053us          3094  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     546.000us         0.00%     546.000us       2.000us           273  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us      22.591ms         0.06%      22.591ms       1.700us         13286  \n",
      "                                    cudaPeekAtLastError         0.00%     158.000us         0.00%     158.000us       0.002us       0.000us         0.00%       0.000us       0.000us         76433  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         1.31%     376.491ms         1.31%     376.491ms       0.434us       0.000us         0.00%       0.000us       0.000us        867751  \n",
      "                                 cudaDeviceGetAttribute         0.00%     178.000us         0.00%     178.000us       0.009us       0.000us         0.00%       0.000us       0.000us         19110  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us      35.022ms         0.09%      35.022ms       2.636us         13286  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      38.695ms         0.10%      38.695ms       2.421us         15981  \n",
      "void compute_cuda_kernel<long>(long*, long*, long*, ...         0.00%       0.000us         0.00%       0.000us       0.000us      21.754ms         0.06%      21.754ms       2.097us         10374  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us     552.000us         0.00%     552.000us       3.033us           182  \n",
      "void (anonymous namespace)::elementwise_kernel_with_...         0.00%       0.000us         0.00%       0.000us       0.000us     542.000us         0.00%     542.000us       1.985us           273  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      11.802ms         0.03%      11.802ms       2.015us          5858  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.955ms         0.02%       5.955ms       2.045us          2912  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us      15.552ms         0.04%      15.552ms       2.749us          5658  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     752.759ms         2.03%     752.759ms       3.418us        220220  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     193.000us         0.00%     193.000us       2.121us            91  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     328.000us         0.00%     328.000us       3.604us            91  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     225.000us         0.00%     225.000us       2.473us            91  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     359.424ms         0.97%     359.424ms       4.614us         77896  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     405.000us         0.00%     405.000us       4.451us            91  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.852ms         0.02%       5.852ms       2.010us          2912  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       7.646ms         0.02%       7.646ms       2.546us          3003  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      12.018ms         0.03%      12.018ms       2.032us          5915  \n",
      "void at::native::(anonymous namespace)::vectorized_l...         0.00%       0.000us         0.00%       0.000us       0.000us     804.930ms         2.17%     804.930ms       5.539us        145327  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_ldg8_f2f_stages_64...         0.00%       0.000us         0.00%       0.000us       0.000us      22.330ms         0.06%      22.330ms      41.048us           544  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     904.366ms         2.43%     904.366ms       6.466us        139867  \n",
      "void gemmSN_TN_kernel<float, 128, 16, 2, 4, 6, 7, fa...         0.00%       0.000us         0.00%       0.000us       0.000us     248.000us         0.00%     248.000us       3.444us            72  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      57.717ms         0.16%      57.717ms       0.766us         75348  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     315.677ms         0.85%     315.677ms       4.358us         72436  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     340.499ms         0.92%     340.499ms       4.872us         69888  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us     705.000us         0.00%     705.000us       2.098us           336  \n",
      "                       ampere_fp16_sgemm_fp16_128x32_nn         0.00%       0.000us         0.00%       0.000us       0.000us     318.000us         0.00%     318.000us       4.417us            72  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      80.437ms         0.22%      80.437ms       5.390us         14924  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_rel...         0.00%       0.000us         0.00%       0.000us       0.000us      22.559ms         0.06%      22.559ms      58.747us           384  \n",
      "void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us        4.901s        13.19%        4.901s      24.109us        203304  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     874.680ms         2.35%     874.680ms       4.025us        217308  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     295.944ms         0.80%     295.944ms       4.086us         72436  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     294.173ms         0.79%     294.173ms       4.061us         72436  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     291.613ms         0.78%     291.613ms       4.026us         72436  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     370.361ms         1.00%     370.361ms       5.113us         72436  \n",
      "void cutlass::Kernel<cutlass_75_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us       6.840ms         0.02%       6.840ms     526.154us            13  \n",
      "                                        cudaMemsetAsync         0.40%     115.198ms         0.40%     115.198ms       6.380us       0.000us         0.00%       0.000us       0.000us         18056  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us      17.376ms         0.05%      17.376ms       0.962us         18056  \n",
      "void at::native::mbtopk::fill<unsigned int, unsigned...         0.00%       0.000us         0.00%       0.000us       0.000us       5.947ms         0.02%       5.947ms       2.042us          2912  \n",
      "void at::native::mbtopk::radixFindKthValues<c10::Hal...         0.00%       0.000us         0.00%       0.000us       0.000us      32.241ms         0.09%      32.241ms       5.536us          5824  \n",
      "void at::native::mbtopk::computeBlockwiseWithinKCoun...         0.00%       0.000us         0.00%       0.000us       0.000us      12.270ms         0.03%      12.270ms       2.107us          5824  \n",
      "void at::native::mbtopk::computeBlockwiseKthCounts<u...         0.00%       0.000us         0.00%       0.000us       0.000us       5.884ms         0.02%       5.884ms       2.021us          2912  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       7.282ms         0.02%       7.282ms       1.250us          5824  \n",
      "void at_cuda_detail::cub::DeviceScanByKeyKernel<at_c...         0.00%       0.000us         0.00%       0.000us       0.000us      18.643ms         0.05%      18.643ms       3.201us          5824  \n",
      "void at::native::mbtopk::gatherTopK<c10::Half, unsig...         0.00%       0.000us         0.00%       0.000us       0.000us      15.947ms         0.04%      15.947ms       5.476us          2912  \n",
      "void at::native::radixSortKVInPlace<-2, -1, 32, 4, c...         0.00%       0.000us         0.00%       0.000us       0.000us      14.696ms         0.04%      14.696ms       5.047us          2912  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       8.791ms         0.02%       8.791ms       3.019us          2912  \n",
      "void at::native::(anonymous namespace)::cunn_SoftMax...         0.00%       0.000us         0.00%       0.000us       0.000us      95.583ms         0.26%      95.583ms      31.829us          3003  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      26.503ms         0.07%      26.503ms       9.101us          2912  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      11.706ms         0.03%      11.706ms       2.010us          5824  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      25.779ms         0.07%      25.779ms       8.853us          2912  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.031ms         0.02%       6.031ms       2.008us          3003  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      21.172ms         0.06%      21.172ms       7.271us          2912  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.842ms         0.02%       5.842ms       2.006us          2912  \n",
      "                                  cudaStreamIsCapturing         0.00%     192.000us         0.00%     192.000us       0.042us       0.000us         0.00%       0.000us       0.000us          4608  \n",
      "void at::native::(anonymous namespace)::distribution...         0.00%       0.000us         0.00%       0.000us       0.000us      11.573ms         0.03%      11.573ms       3.974us          2912  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.560ms         0.02%       6.560ms       2.253us          2912  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      41.366ms         0.11%      41.366ms      14.205us          2912  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.835ms         0.02%       5.835ms       2.004us          2912  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.885ms         0.02%       5.885ms       2.021us          2912  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.840ms         0.02%       5.840ms       2.005us          2912  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.864ms         0.02%       5.864ms       2.014us          2912  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      15.065ms         0.04%      15.065ms       2.547us          5915  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.570ms         0.03%      10.570ms       1.210us          8736  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      11.704ms         0.03%      11.704ms       2.010us          5824  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.921ms         0.01%       2.921ms       2.006us          1456  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.868ms         0.02%       5.868ms       2.015us          2912  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       5.930ms         0.02%       5.930ms       2.036us          2912  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       8.821ms         0.02%       8.821ms       3.029us          2912  \n",
      "void splitKreduce_kernel<32, 16, int, __half, __half...         0.00%       0.000us         0.00%       0.000us       0.000us     559.292ms         1.51%     559.292ms       2.736us        204416  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      52.698ms         0.14%      52.698ms      12.065us          4368  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us     354.479ms         0.95%     354.479ms       5.236us         67704  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us      54.848ms         0.15%      54.848ms       3.126us         17544  \n",
      "void gemv2T_kernel_val<int, int, __half, __half, __h...         0.00%       0.000us         0.00%       0.000us       0.000us        7.796s        20.98%        7.796s      57.576us        135408  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us        5.823s        15.67%        5.823s      86.002us         67704  \n",
      "void gemv2T_kernel_val<int, int, __half, __half, __h...         0.00%       0.000us         0.00%       0.000us       0.000us        1.442s         3.88%        1.442s     511.136us          2821  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.456ms         0.00%       1.456ms       2.000us           728  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     985.406ms         2.65%     985.406ms       7.520us        131040  \n",
      "void gemv2N_kernel<int, int, __half, __half, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us     145.000us         0.00%     145.000us       2.014us            72  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.455ms         0.00%       1.455ms       1.999us           728  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us       2.854ms         0.01%       2.854ms       2.016us          1416  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us       7.917ms         0.02%       7.917ms       2.024us          3912  \n",
      "void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us       2.043ms         0.01%       2.043ms       3.133us           652  \n",
      "void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us     792.000us         0.00%     792.000us       3.000us           264  \n",
      "                                             cudaMalloc         2.21%     637.732ms         2.21%     637.732ms       6.857ms       0.000us         0.00%       0.000us       0.000us            93  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us      12.770ms         0.03%      12.770ms       2.127us          6004  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       2.024ms         0.01%       2.024ms      22.242us            91  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 28.797s\n",
      "Self CUDA time total: 37.153s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = profile_model(SpsModel(model, large_model, 512), {'text_prompt': inputs, 'K':32}, ProfilerConfig(amp=False))\n",
    "print(p.key_averages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ускорение менее заметное, но это было ожидаемо, потому что при больших K acceptence rate маленькой модели мал, что заставляет большую модель делать больше итераций, соответственно замедляя работу алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем K=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-03 10:42:11 1597137:1597137 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2024-05-03 10:43:26 1597137:1597137 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2024-05-03 10:43:28 1597137:1597137 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                       cudaLaunchKernel        50.99%       13.576s        50.99%       13.576s       7.971us       0.000us         0.00%       0.000us       0.000us       1703023  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.095ms         0.02%       7.095ms       2.039us          3479  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     717.000us         0.00%     717.000us       3.983us           180  \n",
      "                                        cudaMemcpyAsync        40.86%       10.879s        40.86%       10.879s     101.627us       0.000us         0.00%       0.000us       0.000us        107050  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us      48.113ms         0.14%      48.113ms       1.002us         48035  \n",
      "                                  cudaStreamSynchronize         6.42%        1.708s         6.42%        1.708s      18.258us       0.000us         0.00%       0.000us       0.000us         93575  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      37.460ms         0.11%      37.460ms       1.665us         22500  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       4.054ms         0.01%       4.054ms       2.047us          1980  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       6.220ms         0.02%       6.220ms       3.141us          1980  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.101ms         0.00%       1.101ms       2.039us           540  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us      36.973ms         0.11%      36.973ms       1.670us         22140  \n",
      "                                    cudaPeekAtLastError         0.02%       4.440ms         0.02%       4.440ms       0.044us       0.000us         0.00%       0.000us       0.000us        101506  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         1.05%     278.355ms         1.05%     278.355ms       0.464us       0.000us         0.00%       0.000us       0.000us        599646  \n",
      "                                 cudaDeviceGetAttribute         0.00%     453.000us         0.00%     453.000us       0.018us       0.000us         0.00%       0.000us       0.000us         25380  \n",
      "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_d...         0.00%       0.000us         0.00%       0.000us       0.000us      56.984ms         0.16%      56.984ms       2.574us         22140  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      40.549ms         0.12%      40.549ms       3.009us         13475  \n",
      "void compute_cuda_kernel<long>(long*, long*, long*, ...         0.00%       0.000us         0.00%       0.000us       0.000us      43.484ms         0.13%      43.484ms       2.119us         20520  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       1.072ms         0.00%       1.072ms       2.978us           360  \n",
      "void (anonymous namespace)::elementwise_kernel_with_...         0.00%       0.000us         0.00%       0.000us       0.000us     990.000us         0.00%     990.000us       1.833us           540  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.701ms         0.02%       6.701ms       2.031us          3299  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.466ms         0.01%       3.466ms       2.140us          1620  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us       8.122ms         0.02%       8.122ms       2.798us          2903  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     905.997ms         2.61%     905.997ms       6.793us        133380  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     384.000us         0.00%     384.000us       2.133us           180  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     705.000us         0.00%     705.000us       3.917us           180  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     458.000us         0.00%     458.000us       2.544us           180  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     419.745ms         1.21%     419.745ms       8.299us         50580  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     828.000us         0.00%     828.000us       4.600us           180  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.288ms         0.01%       3.288ms       2.030us          1620  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       4.987ms         0.01%       4.987ms       2.771us          1800  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.131ms         0.02%       7.131ms       2.085us          3420  \n",
      "void at::native::(anonymous namespace)::vectorized_l...         0.00%       0.000us         0.00%       0.000us       0.000us     519.097ms         1.49%     519.097ms       6.136us         84600  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_ldg8_f2f_stages_64...         0.00%       0.000us         0.00%       0.000us       0.000us      16.547ms         0.05%      16.547ms      25.535us           648  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     579.289ms         1.67%     579.289ms       7.432us         77940  \n",
      "void gemmSN_TN_kernel<float, 128, 16, 2, 4, 6, 7, fa...         0.00%       0.000us         0.00%       0.000us       0.000us     280.000us         0.00%     280.000us       3.889us            72  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      34.253ms         0.10%      34.253ms       0.752us         45540  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     339.577ms         0.98%     339.577ms       7.732us         43920  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     283.591ms         0.82%     283.591ms       7.294us         38880  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us     736.000us         0.00%     736.000us       2.190us           336  \n",
      "                       ampere_fp16_sgemm_fp16_128x32_nn         0.00%       0.000us         0.00%       0.000us       0.000us     348.000us         0.00%     348.000us       4.833us            72  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     161.577ms         0.46%     161.577ms       5.473us         29520  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_rel...         0.00%       0.000us         0.00%       0.000us       0.000us      57.059ms         0.16%      57.059ms     101.168us           564  \n",
      "void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us        2.548s         7.33%        2.548s      24.500us        104004  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us        1.154s         3.32%        1.154s       8.760us        131760  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     397.847ms         1.14%     397.847ms       9.058us         43920  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     395.106ms         1.14%     395.106ms       8.996us         43920  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     390.764ms         1.12%     390.764ms       8.897us         43920  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     550.819ms         1.58%     550.819ms      12.541us         43920  \n",
      "void cutlass::Kernel<cutlass_75_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us       7.883ms         0.02%       7.883ms     525.533us            15  \n",
      "                                        cudaMemsetAsync         0.66%     174.453ms         0.66%     174.453ms       6.617us       0.000us         0.00%       0.000us       0.000us         26363  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us      25.673ms         0.07%      25.673ms       0.974us         26363  \n",
      "void at::native::mbtopk::fill<unsigned int, unsigned...         0.00%       0.000us         0.00%       0.000us       0.000us       3.318ms         0.01%       3.318ms       2.048us          1620  \n",
      "void at::native::mbtopk::radixFindKthValues<c10::Hal...         0.00%       0.000us         0.00%       0.000us       0.000us      18.124ms         0.05%      18.124ms       5.594us          3240  \n",
      "void at::native::mbtopk::computeBlockwiseWithinKCoun...         0.00%       0.000us         0.00%       0.000us       0.000us       7.010ms         0.02%       7.010ms       2.164us          3240  \n",
      "void at::native::mbtopk::computeBlockwiseKthCounts<u...         0.00%       0.000us         0.00%       0.000us       0.000us       3.283ms         0.01%       3.283ms       2.027us          1620  \n",
      "void at_cuda_detail::cub::DeviceScanInitKernel<at_cu...         0.00%       0.000us         0.00%       0.000us       0.000us       4.127ms         0.01%       4.127ms       1.274us          3240  \n",
      "void at_cuda_detail::cub::DeviceScanByKeyKernel<at_c...         0.00%       0.000us         0.00%       0.000us       0.000us      10.636ms         0.03%      10.636ms       3.283us          3240  \n",
      "void at::native::mbtopk::gatherTopK<c10::Half, unsig...         0.00%       0.000us         0.00%       0.000us       0.000us       9.112ms         0.03%       9.112ms       5.625us          1620  \n",
      "void at::native::radixSortKVInPlace<-2, -1, 32, 4, c...         0.00%       0.000us         0.00%       0.000us       0.000us       8.269ms         0.02%       8.269ms       5.104us          1620  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       4.967ms         0.01%       4.967ms       3.066us          1620  \n",
      "void at::native::(anonymous namespace)::cunn_SoftMax...         0.00%       0.000us         0.00%       0.000us       0.000us      58.797ms         0.17%      58.797ms      32.665us          1800  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      14.882ms         0.04%      14.882ms       9.186us          1620  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.520ms         0.02%       6.520ms       2.012us          3240  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      14.516ms         0.04%      14.516ms       8.960us          1620  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.646ms         0.01%       3.646ms       2.026us          1800  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      11.889ms         0.03%      11.889ms       7.339us          1620  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.293ms         0.01%       3.293ms       2.033us          1620  \n",
      "                                  cudaStreamIsCapturing         0.00%     264.000us         0.00%     264.000us       0.082us       0.000us         0.00%       0.000us       0.000us          3218  \n",
      "void at::native::(anonymous namespace)::distribution...         0.00%       0.000us         0.00%       0.000us       0.000us       6.453ms         0.02%       6.453ms       3.983us          1620  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.634ms         0.01%       3.634ms       2.243us          1620  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      23.225ms         0.07%      23.225ms      14.336us          1620  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.287ms         0.01%       3.287ms       2.029us          1620  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.262ms         0.01%       3.262ms       2.014us          1620  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.269ms         0.01%       3.269ms       2.018us          1620  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.293ms         0.01%       3.293ms       2.033us          1620  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       8.939ms         0.03%       8.939ms       2.614us          3420  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.045ms         0.02%       6.045ms       1.244us          4860  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.540ms         0.02%       6.540ms       2.019us          3240  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       1.722ms         0.00%       1.722ms       2.055us           838  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.278ms         0.01%       3.278ms       2.023us          1620  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       3.400ms         0.01%       3.400ms       2.099us          1620  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       5.017ms         0.01%       5.017ms       3.097us          1620  \n",
      "void splitKreduce_kernel<32, 16, int, __half, __half...         0.00%       0.000us         0.00%       0.000us       0.000us     300.162ms         0.86%     300.162ms       2.827us        106192  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     112.210ms         0.32%     112.210ms      12.987us          8640  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us     189.861ms         0.55%     189.861ms       5.494us         34560  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us      25.420ms         0.07%      25.420ms       3.079us          8256  \n",
      "void gemv2T_kernel_val<int, int, __half, __half, __h...         0.00%       0.000us         0.00%       0.000us       0.000us        3.984s        11.46%        3.984s      57.632us         69120  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us        2.973s         8.56%        2.973s      86.039us         34560  \n",
      "void gemv2T_kernel_val<int, int, __half, __half, __h...         0.00%       0.000us         0.00%       0.000us       0.000us     736.128ms         2.12%     736.128ms     511.200us          1440  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us     792.000us         0.00%     792.000us       2.026us           391  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     471.232ms         1.36%     471.232ms       7.792us         60480  \n",
      "void gemv2N_kernel<int, int, __half, __half, __half,...         0.00%       0.000us         0.00%       0.000us       0.000us     149.000us         0.00%     149.000us       2.069us            72  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     789.000us         0.00%     789.000us       2.018us           391  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us       2.775ms         0.01%       2.775ms       2.011us          1380  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       1.435ms         0.00%       1.435ms       7.972us           180  \n",
      "ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f...         0.00%       0.000us         0.00%       0.000us       0.000us      49.781ms         0.14%      49.781ms      47.141us          1056  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     100.151ms         0.29%     100.151ms       2.208us         45360  \n",
      "void at::native::_scatter_gather_elementwise_kernel<...         0.00%       0.000us         0.00%       0.000us       0.000us      20.121ms         0.06%      20.121ms       3.992us          5040  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      43.276ms         0.12%      43.276ms       2.147us         20160  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      68.737ms         0.20%      68.737ms       3.410us         20160  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 26.624s\n",
      "Self CUDA time total: 34.757s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = profile_model(SpsModel(model, large_model, 512), {'text_prompt': inputs, 'K':9}, ProfilerConfig(amp=False))\n",
    "print(p.key_averages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили результат лучше, чем при K=32, но хуже, чем при K=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразим зависимость acceptence_rate от K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [8, 16, 32, 64, 128]\n",
    "acceptence_rates = []\n",
    "for k in K:\n",
    "    res, rate = speculative_sampling(inputs, model, large_model, 1024, k)\n",
    "    acceptence_rates.append(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceptence rate for K=8:  0.895\n",
      "acceptence rate for K=16:  0.8473214285714286\n",
      "acceptence rate for K=32:  0.6360677083333334\n",
      "acceptence rate for K=64:  0.4459821428571429\n",
      "acceptence rate for K=128:  0.338125\n"
     ]
    }
   ],
   "source": [
    "for ind, k in enumerate(K):\n",
    "    print(f'acceptence rate for K={k}: ', acceptence_rates[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Acceptence rate')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(K, acceptence_rates)\n",
    "plt.xlabel(\"K value\")\n",
    "plt.ylabel(\"Acceptence rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "видим, что с ростом K acceptence rate убывает, что очень важно для алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод:\n",
    "\n",
    "В работе я сравнил два метода сэмплирования токенов для языковых моделей, как оказалось, speculative sampling позволяет получить значительный прирост в эффективности, и поэтому может использоваться на практике. Однако, стоит отметить, что speculative sampling требует больше памяти GPU, чем авторегрессия, что приносит свои неудобства в работе"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
